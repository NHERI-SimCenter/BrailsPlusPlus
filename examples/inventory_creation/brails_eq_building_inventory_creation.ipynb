{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a855130-f92a-4e21-ba0a-867d3d35fb4e",
   "metadata": {},
   "source": [
    "## Install BRAILS++ and Folium\n",
    "\n",
    "Before running the following cells, install the BRAILS++ package along with `folium` for interactive mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b355f86-7208-4f58-930d-a9fd21819955",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install brails\n",
    "!pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b547e63-2249-4acd-925d-33724e840987",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c67c37-4094-4402-9fcd-0bdabe0b249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from brails.utils import Importer\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64082d44-b0a3-45a6-a3a3-63fcea03f16f",
   "metadata": {},
   "source": [
    "## Define Location and Scraper Information\n",
    "\n",
    "Specify the target area, output file and number of realizations (worlds) for inventory creation.  \n",
    "\n",
    "- **LOCATION**: The name of the target area (`'Berkeley, CA'`).  \n",
    "- **LOCATION_TYPE**: Specifies how the location is defined, by name (`'locationName'`) or through polygon coordinates (`'locationPolygon'`). \n",
    "- **INVENTORY_OUTPUT**: Filename for saving the retrieved building footprint inventory in GeoJSON format.\n",
    "- **NO_POSSIBLE_WORLDS**: Number of inventory realizations to generate.\n",
    "- **LENGTH_UNIT**: Base unit to be used for attributes that involve length or area measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18195011-9265-4540-a822-d21dd02f5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION_NAME = 'San Francisco California'\n",
    "LOCATION_TYPE = 'locationName'\n",
    "INVENTORY_OUTPUT = 'SFInventory_EQ.geojson'\n",
    "NO_POSSIBLE_WORLDS = 1\n",
    "LENGTH_UNIT = 'ft'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827809a-32d2-4fcf-9fef-dd5c55291f4e",
   "metadata": {},
   "source": [
    "# Create and Importer object to Pull In Required BRAILS Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883970d2-26e5-4872-bde2-2bbc9f09b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = Importer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5ef2e-28f2-4995-a603-10beae7d1e26",
   "metadata": {},
   "source": [
    "## Create a Region Boundary for the Area of Interest\n",
    "\n",
    "We begin by initializing an `Importer` instance, followed by creating a `RegionBoundary` object that defines the target area. Subsequent modules will use the `region_boundary_object` to limit inventory creation to the specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f3301-7c91-484a-ada7-921cbf899496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Importer instance:\n",
    "region_data = {\"type\": LOCATION_TYPE, \"data\": LOCATION_NAME}\n",
    "\n",
    "# Create a region boundary:\n",
    "region_boundary_class = importer.get_class(\"RegionBoundary\")\n",
    "region_boundary_object = region_boundary_class(region_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f2f3a-e8d5-4282-ace7-d298fba12d44",
   "metadata": {},
   "source": [
    "# Get Raw NSI Data for the Defined Region\n",
    "\n",
    "To begin creating our inventory, we first fetch the National Structure Inventory (NSI) data for the specified region using the `NSI_Parser` module in BRAILS++. This data serves as the baseline building attribute information for the area.\n",
    "\n",
    "The `get_raw_data method` retrieves the NSI data exactly as it exists, without applying any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06792f-6b37-41b1-a99f-7c903623ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsi = importer.get_class('NSI_Parser')()\n",
    "nsi_inventory = nsi.get_raw_data(region_boundary_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46785d6-fd76-4322-853c-36ed3c85c9a9",
   "metadata": {},
   "source": [
    "## Create a Building Footprint Scraper and Retrieve the Building Footprints for the Specified Region\n",
    "\n",
    "First, select a scraper class to obtain geometric footprint data for buildings within a given region. Available footprint scraper classes:  \n",
    "- `OSM_FootprintScraper`: Gets OpenStreetMap data  \n",
    "- `USA_FootprintScraper`: Retrieves FEMA USA Structures dataset  \n",
    "- `MS_FootprintScraper`: Uses Microsoft building footprints  \n",
    "- `OvertureMapsFootprintScraper`: Uses Overture Maps data  \n",
    "\n",
    "In the example below, we dynamically load the chosen scraper class, set the output length units to feet, and then request all building footprints within the specified `region_boundary_object`. The result, `footprint_inventory`, contains the geometric data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc180d-9ca9-46a1-8599-ff4747cb4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "footprint_scraper = importer.get_class('OSM_FootprintScraper')({'length': LENGTH_UNIT})\n",
    "footprint_inventory = footprint_scraper.get_footprints(region_boundary_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84256fd8-313f-463a-8371-f1a06ebb8635",
   "metadata": {},
   "source": [
    "# Create a Baseline Inventory by Merging NSI Raw Data and Extracted Footprint Data\n",
    "\n",
    "Next, we combine the NSI data with the building footprint data (`footprint_inventory`) to create a baseline building inventory. This is done using the `get_filtered_data_given_inventory` method.\n",
    "\n",
    "The `get_extended_features` argument allows the inclusion of additional details, such as whether a building is split-level or has a basement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c1cab0-2029-4bd7-ba05-80711381a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsi_inventory = nsi.get_filtered_data_given_inventory(\n",
    "    footprint_inventory, \n",
    "    LENGTH_UNIT, \n",
    "    get_extended_features=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78efa81a-432f-421c-9521-c8e6f2a184c4",
   "metadata": {},
   "source": [
    "# Fill Missing Values in the Baseline Inventory Using KNN Imputation\n",
    "\n",
    "After creating our baseline inventory, some building attributes may still be missing. We use a K-Nearest Neighbors (KNN) approach provided by the BRAILS++ module `KnnImputer`. This step fills in missing values and generates complete inventory realizations.\n",
    "\n",
    "The process to run KNN imputation works as follows:\n",
    "1. **Get the KNN imputer class** using the `Importer`.  \n",
    "2. **Create an imputer instance**, providing:  \n",
    "   - `nsi_inventory`: the baseline NSI inventory  \n",
    "   - `n_possible_worlds`: the number of inventory realizations to generate  \n",
    "   - `exclude_features`: features to skip during imputation (`'lat'`, `'lon'`, `'fd_id'`)  \n",
    "3. **Run the imputer** by calling the `impute` method, which fills in the missing attributes and produces the `imputed_inventory`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c25b0-1693-4462-94ee-94e34bfa4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer_class = importer.get_class('KnnImputer')\n",
    "\n",
    "imputer = knn_imputer_class(\n",
    "    nsi_inventory, \n",
    "    n_possible_worlds=NO_POSSIBLE_WORLDS,\n",
    "    exclude_features=['lat', 'lon', 'fd_id']\n",
    ")\n",
    "imputed_inventory = imputer.impute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5df422-afe3-455a-9460-fe94ea664f92",
   "metadata": {},
   "source": [
    "# Add Household Income Feature Using a Lognormal Distribution\n",
    "To enrich the inventory with socioeconomic data, we add a household income attribute by sampling from a **lognormal distribution**. Income is typically right-skewed, making the lognormal a natural choice.\n",
    "\n",
    "We begin by defining the state average household income (`CA_AVG`) and assuming a 50% coefficient of variation (`CA_STD_DEV`). From these, we calculate the parameters (`mu` and `sigma`) of the underlying normal distribution. Finally, we generate lognormal samples and assign them as the `Income` feature for each building in the imputed inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457f9fc-ee5e-4943-8314-dc4b9b73afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_AVG = 78672  # state average\n",
    "CA_STD_DEV = CA_AVG*0.5  # 50% cov\n",
    "\n",
    "# Step 1: Calculate the parameters of the underlying normal distribution:\n",
    "mu = np.log(CA_AVG**2 /\n",
    "            np.sqrt(CA_STD_DEV**2 + CA_AVG**2))\n",
    "sigma = np.sqrt(np.log(1 + (CA_STD_DEV**2 / CA_AVG**2)))\n",
    "\n",
    "# Step 2: Generate the lognormal sample using the parameters of the normal\n",
    "# distribution:\n",
    "for key, val in imputed_inventory.inventory.items():\n",
    "    lognormal_sample = np.random.lognormal(\n",
    "        mean=mu, \n",
    "        sigma=sigma, \n",
    "        size=NO_POSSIBLE_WORLDS\n",
    "    )\n",
    "    val.add_features({\"Income\": lognormal_sample[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c99a0-15f0-47d9-8288-d510df458ee7",
   "metadata": {},
   "source": [
    "# Change Attribute Keys for Compatibility with R2D \n",
    "\n",
    "In this step, we specify the keys that will be used when enriching the inventory.  Some of these attributes represent new attributes to be inferred, while others correspond to existing attributes that will serve as input to rulesets to predict the derived attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ec6a5-d5d8-4382-8907-5d9e78fd1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names of NEW keys to be inferred:\n",
    "STRUCTURE_TYPE_KEY = 'StructureTypeHazus'      # Instead of  \"constype\" from NSI\n",
    "REPLACEMENT_COST_KEY = 'ReplacementCostHazus'  # Instead of NSI \"repaircost\"\n",
    "\n",
    "# The names of existing keys to be used as \"predictors\":\n",
    "YEAR_BUILT_KEY = 'erabuilt'\n",
    "OCCUPANCY_CLASS_KEY = 'occupancy'\n",
    "INCOME_KEY = 'Income'\n",
    "NUMBER_OF_STORIES_KEY = 'numstories'\n",
    "PLAN_AREA_KEY = 'fpAreas'\n",
    "SPLIT_LEVEL_KEY = 'splitlevel'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0bfdca-831b-4c76-9674-03927477b6de",
   "metadata": {},
   "source": [
    "# Infer Hazus-Compatible Features for Earthquake Analysis\n",
    "\n",
    "With the baseline inventory complete, the next step is to infer Hazus-compatible features that are required for earthquake loss analysis. We use the `HazusInfererEarthquake` class from BRAILS++ to get these attributes.  \n",
    "\n",
    "The `HazusInfererEarthquake` class takes the enriched inventory (in this case `imputed_inventory` and uses predictors such as year built, occupancy class, number of stories, and income to infer key attributes like structure type and replacement cost, which are required for Hazus-style damage and loss analysis. In this example, the `clean_features` argument is set to `False` when intializing the constructor for `HazusInfererEarthquake`, which ensures that both the original predictors and the newly inferred features are retained in hazus_inferred_inventory, rather than limiting the dataset to only the attributes needed for Hazus damage and loss analysis.\n",
    "\n",
    "The resulting `hazus_inferred_inventory`, produced by the `infer` method, is fully aligned with Hazus requirements and ready for use in R2D for regional-scale seismic loss analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74aa9a-692c-4e1a-b575-0fa06c4833f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_features_for_hazuseq = importer.get_class(\"HazusInfererEarthquake\")\n",
    "\n",
    "inferer = infer_features_for_hazuseq(\n",
    "    input_inventory=imputed_inventory,\n",
    "    n_possible_worlds=NO_POSSIBLE_WORLDS,\n",
    "    yearBuilt_key=YEAR_BUILT_KEY,\n",
    "    occupancyClass_key=OCCUPANCY_CLASS_KEY,\n",
    "    numberOfStories_key=NUMBER_OF_STORIES_KEY,\n",
    "    income_key=INCOME_KEY,\n",
    "    splitLevel_key=SPLIT_LEVEL_KEY,\n",
    "    structureType_key=STRUCTURE_TYPE_KEY,\n",
    "    replacementCost_key=REPLACEMENT_COST_KEY,\n",
    "    planArea_key=PLAN_AREA_KEY,\n",
    "    clean_features=False\n",
    ")\n",
    "\n",
    "hazus_inferred_inventory = inferer.infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c10297",
   "metadata": {},
   "source": [
    "From the warning message, inferring HAZUS StructureType for several provided structural types e.g., West Coast-IND1-mid_rise-pre_1950, were not possible using the inference rulestes in BRAILS++, because some provided structural types do not exist in HAZUS's inventory definition. The failed inference has also leaded to missing DesignLevels in the produced inventory here. Below, imputation is used to estimate the HAZUS structural types that do not exist in HAZUS. And the inferrer is run again to estimate DesignLevel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a7dd3-86db-462f-a49c-8d7ca1d0fe0a",
   "metadata": {},
   "source": [
    "# Re-run KNN Imputation to Fill Remaining Missing Values\n",
    "\n",
    "Some attributes in the Hazus-inferred inventory may still be missing because they cannot be determined directly from Hazus rulesets. We use the K-Nearest Neighbors (KNN) imputer to fill these remaining gaps. This step ensures that all missing attributes are estimated, producing a fully populated inventory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d3ea2-c342-48a2-9cfe-ab7caf02ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = knn_imputer_class(\n",
    "    hazus_inferred_inventory, \n",
    "    n_possible_worlds=NO_POSSIBLE_WORLDS\n",
    ")\n",
    "\n",
    "hazus_inferred_inventory_imputed = imputer.impute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce27890-8e32-420a-8d23-c3a6465d52c2",
   "metadata": {},
   "source": [
    "# Generate the Final Hazus-Compatible Inventory for Damage and Loss Analysis\n",
    "\n",
    "In this step, we initialize a new `HazusInfererEarthquake` instance to create the final inventory for Hazus-based damage and loss modeling. We use the `'StructureType'` data from the previous KNN imputation combined with Hazus rulesets to complete the inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98760b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Hazus Inferer for damage and loss analysis (HazusDL):\n",
    "HazusDLInferer = importer.get_class('HazusInfererEarthquake')\n",
    "\n",
    "# Create an instance of the inferer using the imputed Hazus inventory.\n",
    "# In this step, we use the StructureType data obtained from the previous imputation\n",
    "# with the Hazus rulesets to generate the final Hazus-compatible inventory:\n",
    "inferer = HazusDLInferer(\n",
    "    input_inventory=hazus_inferred_inventory_imputed,\n",
    "    n_possible_worlds=NO_POSSIBLE_WORLDS,\n",
    "    yearBuilt_key='erabuilt',\n",
    "    structureType_key='StructureType',\n",
    "    clean_features=False\n",
    ")\n",
    "\n",
    "# Run the inference to produce the final Hazus-compatible inventory:\n",
    "hazus_inventory_final = inferer.infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d905ac6-9b4f-4df6-ae44-0a926cb01aaf",
   "metadata": {},
   "source": [
    "# Change Attribute Names to Make Them Compatible with R2D\n",
    "To prepare the final inventory for R2D analysis, we perform two key steps:\n",
    "1. Rename Features to Match R2D Naming Conventions\n",
    "   - `'erabuilt'` → `'YearBuilt'`  \n",
    "   - `'lat'` → `'Latitude'`  \n",
    "   - `'lon'` → `'Longitude'`  \n",
    "   - `'fpAreas'` → `'PlanArea'`  \n",
    "   - `'numstories'` → `'NumberOfStories'`\n",
    "2. Assign Unique IDs to Each Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f188cb-8e76-452f-adab-5ba98c2acf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename selected features to match R2D naming conventions:\n",
    "hazus_inventory_final.change_feature_names({\n",
    "    'erabuilt': 'YearBuilt',\n",
    "    'lat': 'Latitude',\n",
    "    'lon': 'Longitude',\n",
    "    'fpAreas': 'PlanArea',\n",
    "    'numstories': 'NumberOfStories'\n",
    "})\n",
    "\n",
    "# Assign a unique ID to each building in the inventory:\n",
    "for idx, (_, val) in enumerate(hazus_inventory_final.inventory.items()):\n",
    "    val.add_features({\"id\": idx})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d07bb-b889-4585-ab51-b270662ee89d",
   "metadata": {},
   "source": [
    "# Write the Created Inventory in a GeoJSON File\n",
    "After completing all imputation, inference, and feature standardization steps, we save the final Hazus-compatible inventory to a GeoJSON file for use in R2D or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae677db6-5c02-4a1c-985a-c198835289c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_data = hazus_inventory_final.write_to_geojson(\n",
    "    output_file=INVENTORY_OUTPUT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cc55c-defe-4a28-99d4-74069a3ce2b7",
   "metadata": {},
   "source": [
    "# Plot the Created Inventory\n",
    "This section provides a sample workflow for visualizing the created building inventory on an interactive map. The process consists of the following steps:\n",
    "1. **Extract Footprint Coordinates**: Retrieve the coordinates of all building footprints from the inventory.\n",
    "2. **Flatten Coordinate Lists**: Flatten the nested lists of points from each building footprint into a single list for processing.\n",
    "3. **Calculate Map Center**: Compute the geographic center of all footprints to center the map appropriately.\n",
    "4. **Initialize the Interactive Map**: Create a map centered on the computed location, using a clean, light basemap for clarity.\n",
    "5. **Add Building Footprints with Tooltips**: Add the building footprints as a GeoJSON layer, including tooltips that display key attributes such as income, structure type, year built, and number of stories.\n",
    "6. **Display the Map**: Render the interactive map for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd6acb-4047-428f-b0b1-75b2b4942798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract building footprint coordinates from the inventory\n",
    "inventory_footprints, _ = hazus_inventory_final.get_coordinates()\n",
    "\n",
    "# Flatten the nested coordinate lists into a single list of points\n",
    "all_coords = [coord for path in inventory_footprints for coord in path]\n",
    "\n",
    "# Calculate the geographic center of all footprints for map centering\n",
    "center_lat = sum(point[1] for point in all_coords) / len(all_coords)\n",
    "center_lon = sum(point[0] for point in all_coords) / len(all_coords)\n",
    "\n",
    "# Initialize an interactive map centered on the footprints\n",
    "m = folium.Map(\n",
    "    location=(center_lat, center_lon),\n",
    "    tiles=\"cartodbpositron\",  # Light, clean basemap style\n",
    "    zoom_start=13\n",
    ")\n",
    "\n",
    "# Add building footprints as a GeoJSON layer with tooltips showing \n",
    "folium.GeoJson(\n",
    "    geojson_data,\n",
    "    name=\"geojson\",\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=[\n",
    "            'Income',\n",
    "            'StructureType',\n",
    "            'HeightClass',\n",
    "            'DesignLevel',\n",
    "            'FoundationType',\n",
    "            'OccupancyClass',\n",
    "            'YearBuilt',\n",
    "            'Latitude',\n",
    "            'Longitude',\n",
    "            'NumberOfStories'\n",
    "        ],\n",
    "        sticky=False\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Display the interactive map:\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
