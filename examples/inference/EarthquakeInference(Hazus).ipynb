{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca2c7014-8385-4f36-83c1-00d72a528d5d",
   "metadata": {},
   "source": [
    "# Inferer for Hazus Earthquake Damage and Loss Analysis Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313d40f-da35-49d2-82d0-5d2c93b7ae86",
   "metadata": {},
   "source": [
    "Written by yisangriB, fmk\n",
    "\n",
    "Dec 19, 2024\n",
    "\n",
    "This example demonstrates the final step of Brails++ workflow, which filters in the final feature set that is readily importable in R2D (or Pelicun). For this, a target regional simulation workflow needs to first be specified. In this example, we chose the **Hazus Earthquake Damage and Loss assessment** workflow. \n",
    "\n",
    "Below are six features needed to run **Hazus Earthquake (EQ) Damage and Loss assessment** workflow using R2D .\n",
    "\n",
    "| DL functions             | Feature key                               |\n",
    "|--------------------------|-------------------------------------------|\n",
    "| Building Hazus EQ Damage | StructuralType, BuildingRise, DesignLevel, FoundationType |\n",
    "| Building Hazus EQ Loss   | ReplacementCost, OccupancyClass           |\n",
    "\n",
    "\n",
    "Some features available from NSI, such as StructureType, RepairCost, NumberOfStories, YearBuilt, and OccupancyClass, can be translated to these six features, using the inferer named **Infer_features_for_HazusDL**\n",
    "\n",
    "The outcome inventory can be saved into either a geojson file or a csv file and be readily imported in R2D.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c26585-d0f5-4610-b282-e36dfc734c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written: sy Dec 2024\n",
    "# License: BSD-2\n",
    "\n",
    "\"\"\"\n",
    " Purpose: Testing Inferer\n",
    "\"\"\"\n",
    "\n",
    "# Install packages. They are used only for the map visualization \n",
    "try:\n",
    "    import folium\n",
    "except Exception as e:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install folium\n",
    "    import folium\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "except Exception as e:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install plotly\n",
    "    import plotly.express as px\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import gc # to collect memory\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, \"../../\")\n",
    "from brails.utils import Importer\n",
    "from brails.types.image_set import ImageSet    \n",
    "from brails.types.asset_inventory import Asset, AssetInventory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae0f1d-1a6d-4e62-a7ec-4e4413e0cdf0",
   "metadata": {},
   "source": [
    "## Step1: Set the regional boundary by its name\n",
    "\n",
    "Target city is Berkeley, CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d1435-b878-4250-bc5d-b9ac8c592385",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data = {\"type\": \"locationName\", \"data\": \"Berkeley, CA\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce82a73-2ba4-4668-8cd7-763323289639",
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = Importer()\n",
    "region_boundary_class = importer.get_class(\"RegionBoundary\")\n",
    "region_boundary_object = region_boundary_class(region_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6118886-f1e1-4a5a-a18d-4bbc08fb54a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the regional bound\n",
    "bound = region_boundary_object.get_boundary()\n",
    "bound[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9506a5-9f10-4372-b631-9cd5a132552d",
   "metadata": {},
   "source": [
    "Set the target number of possible worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b1fc7-1f51-43d8-8e4a-3fb78ef4a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pw = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5e3b8-1f98-42d0-ba8b-808287c80b1b",
   "metadata": {},
   "source": [
    "## Step2: Obtain baseline inventory data by merging OSM foot print with NSI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c79c0d-9ae9-41dd-9155-56cbb7b98898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape NSI database\n",
    "nsi_class = importer.get_class('NSI_Parser')\n",
    "nsi = nsi_class()\n",
    "nsi_inventory = nsi.get_raw_data(region_boundary_object)\n",
    "\n",
    "print('Total number of assets detected using NSI is '\n",
    "      f'{len(nsi_inventory.inventory)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7bcea-6cf1-4d43-a330-1895cec608e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the building inventory from OSM:\n",
    "scraper_class = importer.get_class('OSM_FootprintScraper')\n",
    "scraper = scraper_class({'length': 'ft'})\n",
    "scraper_inventory = scraper.get_footprints(region_boundary_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a3a655-a809-4721-9a00-6790dec6ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge these footprints with the NSI raw data previously downloaded:\n",
    "# Remeber to set \"get_extended_features\" to true to get split-level and basement information - this will be used in Section 5\n",
    "new_inventory = nsi.get_filtered_data_given_inventory(scraper_inventory, \"ft\", get_extended_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dedaa61-63a9-4faf-96ea-a8481153970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Asset id=6, After Merging NSI and OSM\")\n",
    "new_inventory.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5a251-920b-4b53-8089-5764b3a7ee5f",
   "metadata": {},
   "source": [
    "## Step 3. Run imputation to fill in missing values\n",
    "\n",
    "Imputer fills in missing values when a feature is available for a subset of buildings in a region but not for all buildings, and it imputes the values by assuming a similarity between neighboring buildings. Therefore, the assumption is that the feature is available at least for a subset of buildings, and those buildings provide reliable information of the neighboring buildings.\n",
    "\n",
    "If a feature is completely missing for the entire building stock in the area, that feature cannot be 'imputed.' Such features may be 'inferred' using external rulesets (Hazus or user-provided). See Step 5 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4be25-9d23-48ef-aaa6-2d2c5f17393c",
   "metadata": {},
   "source": [
    "### Importing imputer and run imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c847a-f75f-4d2d-bb51-18b866c7ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = Importer()\n",
    "knn_imputer_class = importer.get_class(\"KnnImputer\")\n",
    "\n",
    "imputer=knn_imputer_class(new_inventory,n_possible_worlds=n_pw,exclude_features=['lat','lon','fd_id'])\n",
    "new_inventory = imputer.impute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7cda0f-280f-4480-bbd1-3321d6d37cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Asset id=6, After imputation\")\n",
    "new_inventory.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb576d99-0a81-459a-8d48-808e092902b5",
   "metadata": {},
   "source": [
    "## Step 4. Collect Income information (Let's randomly generate it for now)\n",
    "\n",
    "At this point, Brails ++ does not automatically scrape the household income information. The user can import it from a CSV file or use a user-defined inferer to augment the information.\n",
    "For now, let's just randomly generate it from a lognormal distribution. The income information source will be integrated into brails++ in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c0947-ab8d-475d-a918-1afa5d3b017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary add incomes\n",
    "import numpy as np\n",
    "\n",
    "cal_avg = 78672 # state average\n",
    "std_dev = cal_avg*0.5 # 50% cov\n",
    "\n",
    "mean_original = cal_avg     # Mean in the original space (lognormal)\n",
    "std_dev_original = std_dev     # Standard deviation in the original space (lognormal)\n",
    "sample_size = 1         # Number of samples to generate\n",
    "# Step 1: Calculate the parameters of the underlying normal distribution\n",
    "mu = np.log(mean_original**2 / np.sqrt(std_dev_original**2 + mean_original**2))\n",
    "sigma = np.sqrt(np.log(1 + (std_dev_original**2 / mean_original**2)))\n",
    "\n",
    "# Step 2: Generate the lognormal sample using the parameters of the normal distribution\n",
    "for key, val in new_inventory.inventory.items():\n",
    "    lognormal_sample = np.random.lognormal(mean=mu, sigma=sigma, size=sample_size)\n",
    "    val.add_features({\"Income\":lognormal_sample[0]})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c88335-bf9a-4b31-ae3d-ef6870bde8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Asset id=6, After adding income\")\n",
    "new_inventory.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770d75c-8179-4e06-9a11-738d0f4be4cb",
   "metadata": {},
   "source": [
    "## Step 5. Basic inference using NSI data\n",
    "\n",
    "Recall that below are six features that are needed to run Hazus EQ Damage and Loss assessment workflow using R2D (Pelican).\n",
    "\n",
    "| DL functions          | Feature key                               |\n",
    "|-----------------------|-------------------------------------------|\n",
    "| Building Hazus EQ Damage | StructuralType, BuildingRise, DesignLevel, FoundationType |\n",
    "| Building Hazus EQ Loss   | ReplacementCost, OccupancyClass           |\n",
    "\n",
    "\n",
    "Among the six features, \n",
    "* **StructuralType, ReplacementCost, OccupancyClass** are already provided from NSI, so we need to simply change their key names.\n",
    "* **BuildingRise, DesignLevel, FoundationType** require a simple deterministic inference, taking NumberOfStories, YearBuilt, and StructuralType as inputs\n",
    "\n",
    "The inferer module named **Infer_features_for_HazusDL** will take care of both tasks as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa2355-21e8-4f07-8451-cb8e9905094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing key names\n",
    "structureType_key = 'constype'\n",
    "replacementCost_key = 'repaircost'\n",
    "numberOfStories_key = 'numstories'\n",
    "yearBuilt_key = 'erabuilt'\n",
    "occupancyClass_key = 'occupancy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74079f57-dddf-44a3-933d-05fce267bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Infer_features_for_HazusDL2 = importer.get_class(\"HazusInfererEarthquake\")\n",
    "inferer=Infer_features_for_HazusDL2(input_inventory=new_inventory, \n",
    "                                n_possible_worlds=n_pw, \n",
    "                                yearBuilt_key = yearBuilt_key, \n",
    "                                occupancyClass_key = occupancyClass_key, \n",
    "                                numberOfStories_key = numberOfStories_key, \n",
    "                                structureType_key = structureType_key,\n",
    "                                replacementCost_key = replacementCost_key,\n",
    "                                clean_features= True)\n",
    "new_inventory_HazusDL = inferer.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a57eb-0c24-4f30-a765-edc5c558dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Asset id=6, After ruuning Inferer\")\n",
    "new_inventory_HazusDL.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c5d61-a9ad-4c47-8979-5cccc31aa76e",
   "metadata": {},
   "source": [
    "Note that the replacement cost of this building obtained from NSI is zero. If you think this is not reliable, you can also utilize Hazus inferer to improve this. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9833e705-475b-4152-a2a8-9b16270e7e89",
   "metadata": {},
   "source": [
    "## Step 5 (alternative). Run advanced inference using Hazus rulesets\n",
    "\n",
    "Alternativly, one can additionally estimate **StructuralType** and/or **ReplacementCost** using rulesets provided in the Hazus inventory manual 6, instead of relying on NSI dataset. This is particularly useful when user have a more reliable, high-resolution local/private data sources (e.g. for YearBuilt, Income, NumberOfStories etc.) that may provide a better information to evaluate the **StructuralType** and **ReplacementCost**. To enable this feature, provide \"new\" key names for these two features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e7844-6adc-4758-b8cb-2199e4e5fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names of NEW keys to be infered.\n",
    "structureType_key = 'StructureTypeHazus' # instead of  \"constype\" from NSI\n",
    "replacementCost_key = 'ReplacementCostHazus' # instead of \"repaircost\" from NSI\n",
    "\n",
    "# The names of existing keys to be used as \"predictors\"\n",
    "yearBuilt_key = 'erabuilt'\n",
    "occupancyClass_key = 'occupancy'\n",
    "income_key = 'Income'\n",
    "numberOfStories_key = 'numstories'\n",
    "planArea_key = 'fpAreas'\n",
    "splitLevel_key = 'splitlevel'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257b0dd-cd61-4635-912e-32950a1804f7",
   "metadata": {},
   "source": [
    "Note that only if these keys for **StructuralType** and **ReplacementCost** don't exist in the current inventory file, the module will automatically infer it. To make the inference, additional inventory is needed, such as **Income**, **PlanArea**, and **SplitLevel**. See the user manual to understand what feature keys need to be included as predictors to infer what feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ed406-dd30-49c0-aa7a-c1c2b9b2822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Infer_features_for_HazusDL = importer.get_class(\"HazusInfererEarthquake\")\n",
    "inferer=Infer_features_for_HazusDL(input_inventory=new_inventory, \n",
    "                                n_possible_worlds=n_pw, \n",
    "                                yearBuilt_key = yearBuilt_key, \n",
    "                                occupancyClass_key = occupancyClass_key, \n",
    "                                numberOfStories_key = numberOfStories_key, \n",
    "                                income_key=income_key, \n",
    "                                splitLevel_key = splitLevel_key,\n",
    "                                structureType_key = structureType_key,\n",
    "                                replacementCost_key = replacementCost_key,\n",
    "                                planArea_key=planArea_key,\n",
    "                                clean_features= False)\n",
    "new_inventory_HazusDL2 = inferer.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550577fd-d56c-4587-add2-421f7bf3c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inventory_HazusDL2.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4e44b-ab67-40a0-8f09-371c985b8254",
   "metadata": {},
   "source": [
    "Some instances of **StructuralType**  are missing in the final attributes. As instructed by the warning msg, let's do some imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85254aed-4d7b-400e-a400-155e679d109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer=knn_imputer_class(new_inventory_HazusDL2,n_possible_worlds=n_pw)\n",
    "new_inventory_HazusDL2_imputed = imputer.impute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48beb915-a463-4774-bbae-383135289d0a",
   "metadata": {},
   "source": [
    "Now let's run inferer again. Make sure to specify the name \"**StructureType**\" so that imputed values are utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3487595-2f7a-4225-b13f-9d486de39ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will keep StructureType we just imputed\n",
    "\n",
    "Infer_features_for_HazusDL = importer.get_class(\"HazusInfererEarthquake\")\n",
    "inferer=Infer_features_for_HazusDL(input_inventory=new_inventory_HazusDL2_imputed, \n",
    "                                n_possible_worlds=n_pw, \n",
    "                                yearBuilt_key = yearBuilt_key, \n",
    "                                structureType_key = 'StructureType',\n",
    "                                clean_features= True)\n",
    "new_inventory_HazusDL2 = inferer.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e0839-1a16-40fa-841a-9e4942c65db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inventory_HazusDL2.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc84ea-e389-4d47-835c-82893306c74d",
   "metadata": {},
   "source": [
    "## Step 7: Visualize results\n",
    "The original inventory is too big. Let's select a random subset of buildings with size of 2395 to save some memory space for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6bd66-95de-4a4d-ba6a-45186e85da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_hazus_subset = new_inventory_HazusDL2.get_random_sample(nsamples=2395, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51dfc6-3e19-45c1-8c59-c461796668c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color rules - for simplicity let's group the structural type by material\n",
    "category_colors = {\n",
    "    'W1': 'blue',\n",
    "    'W2': 'blue',\n",
    "    'W': 'blue',\n",
    "    'RM1': 'green',\n",
    "    'RM2': 'green',\n",
    "    'M': 'green',\n",
    "    'C1': 'orange',\n",
    "    'C2': 'orange',\n",
    "    'C3': 'orange',\n",
    "    'C': 'orange',\n",
    "    'S1': 'black',\n",
    "    'S2': 'black',\n",
    "    'S3': 'black',\n",
    "    'S4': 'black',\n",
    "    'S5': 'black',\n",
    "    'S': 'black',\n",
    "    'PC1': 'pink',\n",
    "    'PC2': 'pink',\n",
    "    'MH': 'red',\n",
    "    'URM': 'grey',\n",
    "    'H1': 'grey',\n",
    "    'H': 'grey',\n",
    "    'nan': 'yellow',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2359e588-66f4-42a9-840a-40e0bd19c80c",
   "metadata": {},
   "source": [
    "Below are some handy visualization functions to draw polygon/scatter maps from brails inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd45e0-3dbf-4185-9675-43c5555b7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def geo_polygon(new_inventory_simcenter, feature_key, feature_type, pw_id=0, str_colors=None, min_value=None, max_value=None, title=\"\"):\n",
    "\n",
    "    print(title)\n",
    "    new_inventory_re1=new_inventory_simcenter.get_world_realization(pw_id)\n",
    "\n",
    "    inventory_footprints = new_inventory_re1.get_coordinates()\n",
    "    geojson_data = new_inventory_re1.get_geojson()\n",
    "    # print(list(geojson_data['features'][0]['properties']))\n",
    "\n",
    "    feature_list = list(geojson_data['features'][0]['properties'])\n",
    "    \n",
    "    def get_global_centroid(list_of_polygons):\n",
    "        all_points = [point for polygon in list_of_polygons for point in polygon]\n",
    "        return (sum(x for x, _ in all_points) / len(all_points), sum(y for _, y in all_points) / len(all_points))\n",
    "\n",
    "    \n",
    "    center_tmp = get_global_centroid(inventory_footprints[0])\n",
    "    center = (center_tmp[1],center_tmp[0])\n",
    "    #center = (37.64, -122.09)\n",
    "    if feature_type==\"float\":\n",
    "        log_colormap = folium.LinearColormap(['green', 'yellow', 'red'], vmin=np.log(min_value), vmax=np.log(max_value))\n",
    "        def style_function(feature):\n",
    "            feature_val = feature['properties'].get(feature_key, min_value)  # Default to min_value if not found\n",
    "            log_feature_val = np.log(feature_val)  # Apply the logarithmic transformation\n",
    "            color = log_colormap(log_feature_val)  # Get the color from the log scale colormap\n",
    "            return {'fillColor': color, 'color': color, 'weight': 2, 'fillOpacity': 0.5}\n",
    "            \n",
    "    elif feature_type==\"str\":\n",
    "        def style_function(feature):\n",
    "            # Get the category from the feature properties (assuming the category is stored under the 'category' key)\n",
    "            category = feature['properties'].get(feature_key, 'nan')  # Default to 'nan' if no category is found\n",
    "            color = str_colors.get(category, 'yellow')\n",
    "            return {'fillColor': color, 'color':color , 'weight': 2, 'fillOpacity': 0.5}\n",
    "\n",
    "    m = folium.Map(location=center, tiles=\"cartodbpositron\", zoom_start=12)\n",
    "    g = folium.GeoJson(\n",
    "        geojson_data,\n",
    "        name=\"geojson\",\n",
    "        tooltip=folium.GeoJsonTooltip(fields=feature_list, sticky=False), # features of the first asset\n",
    "        style_function=style_function\n",
    "    ).add_to(m)\n",
    "\n",
    "    from IPython.display import display, IFrame, HTML\n",
    "\n",
    "    display(HTML(f\"\"\"\n",
    "        <div style=\"width: 600px; height: 450; border: 2px solid black; padding: 10px;\">\n",
    "            {m._repr_html_()}\n",
    "        </div>\n",
    "    \"\"\"))\n",
    "    gc.collect()\n",
    "    return \n",
    "\n",
    "def geo_scatter(new_inventory_simcenter, feature_key, feature_type, pw_id=0, str_colors=None, float_log = None, min_value=None, max_value=None, title=\"\"):\n",
    "    new_inventory_re1=new_inventory_simcenter.get_world_realization(pw_id)\n",
    "    inventory_new_df, geom_new_df, nbldg = new_inventory_re1.get_dataframe()\n",
    "    feature_list = list(inventory_new_df.columns)\n",
    "    #feature_list.remove('lat')\n",
    "    #feature_list.remove('lon')\n",
    "    if feature_type==\"float\":\n",
    "\n",
    "        if float_log:\n",
    "            log_values = np.log(inventory_new_df[feature_key].astype(float) + 1e-6)\n",
    "        else:\n",
    "            log_values = inventory_new_df[feature_key].astype(float)\n",
    "            \n",
    "        colorscale = [\n",
    "            [0, 'green'],   # 0 (min value)\n",
    "            [0.5, 'yellow'], # Midpoint (log scale)\n",
    "            [1, 'red']      # 1 (max value)\n",
    "        ]\n",
    "        fig = px.scatter_mapbox(inventory_new_df, \n",
    "                               lat=geom_new_df[\"Lat\"], \n",
    "                               lon=geom_new_df[\"Lon\"], \n",
    "                               color = log_values,\n",
    "                               zoom=11, \n",
    "                               mapbox_style='open-street-map',\n",
    "                               hover_data=feature_list,\n",
    "                               color_continuous_scale = colorscale,\n",
    "                               width=600, \n",
    "                               height=450,\n",
    "                               title = title)\n",
    "\n",
    "        if float_log:\n",
    "            fig.update_layout(\n",
    "                coloraxis_colorbar=dict(title=f\"logscale\",),\n",
    "                coloraxis=dict(cmin=np.log(min_value), cmax=np.log(max_value)),\n",
    "            )\n",
    "    \n",
    "    elif feature_type==\"str\":\n",
    "        fig = px.scatter_mapbox(inventory_new_df, \n",
    "                                lat=geom_new_df[\"Lat\"], \n",
    "                                lon=geom_new_df[\"Lon\"], \n",
    "                                color=inventory_new_df[feature_key].astype(str), \n",
    "                                range_color=[-1, 4], \n",
    "                                zoom=11, \n",
    "                                mapbox_style='open-street-map',\n",
    "                                color_discrete_map=str_colors,\n",
    "                                width=600, \n",
    "                                height=450,\n",
    "                                hover_data=feature_list,\n",
    "                               title = title)\n",
    "    fig.show()\n",
    "    gc.collect()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b4d71-4044-404d-bca5-a0e8e7278efa",
   "metadata": {},
   "source": [
    "## Visualizing StructureType and ReplacementCost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80007f-ac6a-4b0f-970e-22a5c652d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_scatter(inventory_hazus_subset, 'StructureType', 'str', str_colors = category_colors, title=\"StructureType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c945913-5196-4d4b-bcbb-35b48f63a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_scatter(inventory_hazus_subset, 'ReplacementCost', 'float', min_value=5.e4, max_value=1.e8, title = 'ReplacementCost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be821f-2e38-4fe8-9856-bc18d2a884f6",
   "metadata": {},
   "source": [
    "## Step 8: Save the inventory into a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c37bf-4d0a-432a-9be2-1ce5176d1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inventory_HazusDL2.convert_polygons_to_centroids() # to save some memory space\n",
    "_ = new_inventory_HazusDL2.write_to_geojson('Berkeley_building.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c02eb2-a401-4808-84cf-2bdae11e2cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
